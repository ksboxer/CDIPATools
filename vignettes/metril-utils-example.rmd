---
title: "Metrics Utils Example"
author: "Zarni Htet + Kate Boxer"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
library(CDIPATools)
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```
## Read in Sample Dataset

This dataset is contained in the data file of this package.  This dataset has been made to simulate what predictions would look like from a model, meaning it contains a column that has probability scores, and also a column that has a prediction (0/1) for binary classification, and also has a column that contains a true label that is binary

```{r}

## ZARNI : How do I do paths within the package?
fake_predictions_data <- read.csv("~/CDIPATools/data/FakePredictionResults.csv",stringsAsFactors = FALSE)
head(fake_predictions_data)
```
We will use the "true.risk.bin" column as true labels, and the column "est.risk.score" as the predicted probability by a model

Now we will walk through and show each function in the "~/R/utils/metrics.R" file works

## get_confusion_matrix

We will start by showing how the get confusion metric function works.  We will show that by using a cut-off threshold of .5 for the prediction probabilities

```{r}
get_confusion_matrix(predictions = fake_predictions_data$est.risk.score, labels = fake_predictions_data$true.risk.bin, threshold = .5)
```
## get_fpr

We will show how the get false positive rate function works.  We will show that by using a cut-off threshold of .5 for the prediction probabilities

```{r}
get_fpr(predictions = fake_predictions_data$est.risk.score, labels = fake_predictions_data$true.risk.bin, threshold = .5)
```
## get_fnr

We will show how the get false negative rate function works.  We will show that by using a cut-off threshold of .5 for the prediction probabilities

```{r}
get_fnr(predictions = fake_predictions_data$est.risk.score, labels = fake_predictions_data$true.risk.bin, threshold = .5)
```
## get_tpr

We will show how the get true positive rate function works.  We will show that by using a cut-off threshold of .5 for the prediction probabilities

```{r}
get_tpr(predictions = fake_predictions_data$est.risk.score, labels = fake_predictions_data$true.risk.bin, threshold = .5)
```
## get_tnr

We will show how the get true negative rate function works.  We will show that by using a cut-off threshold of .5 for the prediction probabilities

```{r}
get_tnr(predictions = fake_predictions_data$est.risk.score, labels = fake_predictions_data$true.risk.bin, threshold = .5)
```
## get_auc_roc

We will show how the get auc roc function works.

```{r}
get_auc_roc(predictions = fake_predictions_data$est.risk.score, labels = fake_predictions_data$true.risk.bin)
```
## get_auc_pr

We will show how the get auc precision recall curve function works.

```{r}
get_auc_pr(predictions = fake_predictions_data$est.risk.score, labels = fake_predictions_data$true.risk.bin)
```
## get_precision

We will show how the get precision function works.  We will show that by using a cut-off threshold of .5 for the prediction probabilities

```{r}
get_precision(predictions = fake_predictions_data$est.risk.score, labels = fake_predictions_data$true.risk.bin, threshold = .5)
```
## get_recall

We will show how the get recall function works.  We will show that by using a cut-off threshold of .5 for the prediction probabilities

```{r}
get_recall(predictions = fake_predictions_data$est.risk.score, labels = fake_predictions_data$true.risk.bin, threshold = .5)
```
## get_accuracy

We will show how the get precision function works.  We will show that by using a cut-off threshold of .5 for the prediction probabilities

```{r}
get_accuracy(predictions = fake_predictions_data$est.risk.score, labels = fake_predictions_data$true.risk.bin, threshold = .5)
```
## threshold_finder

The threshold_finder function finds the optimal threshold cut-off for predictions based on a passed in metric function.  All the passed in metric functions need to take the parameters predictions,labels,and threshold.  We will show how to find the optimal threshold to maximize precision
```{r}
threshold_finder(get_precision, predictions = fake_predictions_data$est.risk.score, labels = fake_predictions_data$true.risk.bin)
```
We will show how to find the optimal threshold to maximize recall

```{r}
threshold_finder(get_recall, predictions = fake_predictions_data$est.risk.score, labels = fake_predictions_data$true.risk.bin)
```
We will show how to find the optimal threshold to maximize accuracy

```{r}
threshold_finder(get_accuracy, predictions = fake_predictions_data$est.risk.score, labels = fake_predictions_data$true.risk.bin)
```
