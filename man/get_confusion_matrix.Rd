% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/metrics.R
\name{get_confusion_matrix}
\alias{get_confusion_matrix}
\title{This function produces a confusion matrix - a table that displays the false positive (FP), false negative (FN), true positive(TP), and true negative (results) by comparing a set of predictions to true values.
The predictions can either be binary or continuous. For continuous predictions, a threshold for translating the results to binary classifications must be supplied. If the predictions are already binary, then pass in .5. (KP: Why not just null?)}
\usage{
get_confusion_matrix(predictions, labels, threshold)
}
\arguments{
\item{predictions}{list (KP: isn't it a numeric vector?) of numerics,  predicted values}

\item{labels}{(KP: let's limit jargon and say call this "outcomes" or "trueOutcomes" list of numerics (KP: again use correct R object), actual values/outcomes (KP: if binary, need to be 0/1 yes?)}

\item{threshold}{numeric, value between 0 and 1 to translate continuous predictions to binary classifications}
}
\value{
table (KP: is this an object name or object classification?), table of integers with FP, FN, TP, TN.
}
\description{
This function produces a confusion matrix - a table that displays the false positive (FP), false negative (FN), true positive(TP), and true negative (results) by comparing a set of predictions to true values.
The predictions can either be binary or continuous. For continuous predictions, a threshold for translating the results to binary classifications must be supplied. If the predictions are already binary, then pass in .5. (KP: Why not just null?)
}
