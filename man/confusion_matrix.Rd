% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/predictiveperformancemetrics.R
\name{confusion_matrix}
\alias{confusion_matrix}
\title{This function produces a confusion matrix - a table that displays the false positive (FP), false negative (FN), true positive(TP), and true negative (results) by comparing a set of predictions to true values.
The predictions can either be binary or continuous. For continuous predictions, a threshold for translating the results to binary classifications must be supplied. If the predictions are already binary, then pass in .5. (KP: Why not just null?)}
\usage{
confusion_matrix(predictions, outcomes, threshold)
}
\arguments{
\item{predictions}{vector of numerics, predicted values}

\item{outcomes}{vector of numerics, actual values/outcomes, if binary we need to a 0/1}

\item{threshold}{numeric, value between 0 and 1 to translate continuous predictions to binary classifications}
}
\value{
list returns a list object that includes a confusion matrix table, accuracy, kappa statistics etc
}
\description{
This function produces a confusion matrix - a table that displays the false positive (FP), false negative (FN), true positive(TP), and true negative (results) by comparing a set of predictions to true values.
The predictions can either be binary or continuous. For continuous predictions, a threshold for translating the results to binary classifications must be supplied. If the predictions are already binary, then pass in .5. (KP: Why not just null?)
}
\examples{
confusion_matrix(predictions = FakePredictionResults$est.risk.score,
outcomes = FakePredictionResults$true.risk.bin, threshold = 0.5)
}
