% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/predictiveperformancemetrics.R
\name{accuracy}
\alias{accuracy}
\title{Calculate the Accuracy for predictions}
\usage{
accuracy(predictions, outcomes, threshold)
}
\arguments{
\item{predictions}{list of numerics,  predicted values}

\item{outcomes}{list of numerics, actual values/outcomes}

\item{threshold}{numeric, value between 0 - 1 to cut  predictions that are continous within binary 0s and 1s}
}
\value{
numeric, returns accuracy value
}
\description{
This function takes the predictions of a model, (can be either binary 0 or 1, or continous numeric [0,1]) and
calculates the accuracy based on the predictions.  Given that predictions need to binary for the accuracy calculates
you need to pass in a threshold to but the predictions off.  If the predictions are already binary, then pass in .5
Note: if there are imbalanced positives and negatives this metric might not be that useful
}
\examples{
accuracy(predictions = FakePredictionResults$est.risk.score,
outcomes = FakePredictionResults$true.risk.bin, threshold = .5)
}
